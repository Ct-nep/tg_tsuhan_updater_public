# !/usr/bin/python3

import sqlite3
import time
import logging
from abc import abstractmethod
from datetime import datetime
from concurrent import futures
from typing import Any

import requests
from requests.adapters import HTTPAdapter, Retry
from pytz import timezone

logger = logging.getLogger(__name__)

class CrawlerBase:

    def __init__(self, threads: int):
        logger.info(f'Start {self.__class__.__name__}')
        self.error_count = 0
        self._s = requests.Session()
        self._s.headers.update({
            'Accept': \
                'text/html,application/xhtml+xml,application/xml;'\
                'q=0.9, image/avif,image/webp,*/*;q=0.8',
            'Accept-Language': 'ja-JP',
            'User-Agent': \
                'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:99.0) '\
                'Gecko/20100101 Firefox/99.0',
            })
        retries = Retry(
            total=3,
            backoff_factor=0.2,
            status_forcelist=[429, 500, 502, 503, 504]
        )
        self._s.mount('http://', HTTPAdapter(max_retries=retries))
        self._s.mount('https://', HTTPAdapter(max_retries=retries))
        self.executor = futures.ThreadPoolExecutor(threads)

    def __del__(self):
        self.executor.shutdown()
        logger.info(f'Terminate {self.__class__.__name__}.')

    @abstractmethod
    def get_one(self, keyword: str, page: int, timeout: int) -> str:
        '''Get one page given specific keyword and page number. Return
        request content (bytes) for further tweak.'''

    @abstractmethod
    def get_many(self, keywords: list) -> list:
        '''Get all possible pages for each keyword in supplied list. It
        is recommend to return a dict instead of list, indicating
        page_no and other info.'''

    @staticmethod
    @abstractmethod
    def _get_params(keyword: str, page: int) -> dict:
        '''Construct request parameters. Return a dict of params.'''


class ManagerBase:

    def __init__(self, db_path: str=':memory:'):
        self.db_path = db_path
        logger.info(f'Start {self.__class__.__name__}.')

    def __del__(self):
        self.close()
        logger.info(f'Terminate {self.__class__.__name__}. '\
            f'Connection closed.')

    def __repr__(self):
        return f'{self.__class__.__name__}(db_path={self.db_path})'

    @property
    def db_path(self):
        return self._db_path

    @db_path.setter
    def db_path(self, path):
        self._db_path = path
        self.close()
        self._con = sqlite3.connect(self.db_path)
        self._con.row_factory = sqlite3.Row
        logger.info(f'{self.__class__.__name__} set to db path: {path}')

    def check_exist(self, table_name: str, overwrite=False) -> bool:
        '''Check if given table exists, return boolean value.
        If overwrite is set to True, will always delete existing
        table and return False.
        '''
        try:
            sql = f'SELECT * FROM {table_name}'
            if not self._con.execute(sql).fetchone() or overwrite:
                raise sqlite3.DataError
        except sqlite3.DataError:
            sql = f'DROP TABLE {table_name}'
            self._con.execute(sql)
            self._con.commit()
            return False
        except sqlite3.OperationalError:
            return False
        else:
            return True

    def find_id(self, table: str, item_id: Any) -> sqlite3.Row:
        '''Find entry in a site table matched by item_id.'''
        try:
            sql = f'SELECT * FROM {table} WHERE item_id = ?'
            q = self._con.execute(sql, [item_id]).fetchone()
            return q
        except sqlite3.Error as exc:
            logger.error(f'Cannot find entry by its id. Exc: {exc}')
            return None

    def close(self):
        try:
            self._con.commit()
            self._con.close()           
        except Exception:
            pass

    @staticmethod
    def from_timestamp(t: float, tz_name='Asia/Tokyo') -> str:
        tz = timezone(tz_name)
        return datetime.fromtimestamp(t, tz).isoformat(' ', 'seconds')


class CrawlerManager(ManagerBase):

    def __init__(self, keywords: list, db_path=':memory:'):
        super().__init__(db_path)
        self.keywords = keywords
        self.info = {'time' : time.time()}
        self._create_log()

    def __repr__(self):
        return f'{self.__class__.__name__}({self.db_path})'

    @abstractmethod
    def _create_table(self) -> None:
        '''Create a table to do the job.'''

    @abstractmethod
    def _to_table(self, data: list) -> None:
        '''Pack item info generated by the crawler into a table.'''

    def _create_log(self) -> None:
        '''Create log table containing historical update info, if
        missing.
        '''
        if not self.check_exist('log'):
            sql = \
                'CREATE TABLE log'\
                '(id INTEGER PRIMARY KEY AUTOINCREMENT,'\
                'site TEXT,'\
                'time REAL,'\
                'error INTEGER,'\
                'page INTEGER,'\
                'count INTEGER,'\
                'new INTEGER,'\
                'discount INTEGER,'\
                'sold INTEGER)'
            self._con.execute(sql)
            self._con.commit()

    def _get_update_time(self) -> tuple:
        '''Set the 'last' entry in self.info, reflecting time of last 
        stable update.
        '''
        sql = \
            'SELECT time FROM log '\
            'WHERE site = ? AND error = 0 '\
            'ORDER BY time DESC'
        query = self._con.execute(sql, [self.info['site']]).fetchone()
        self.info['last'] = query['time'] if query else 0.0

    @staticmethod
    def _split(msg: str) -> list:
        r'''Split strings longer than 4096 chars by closest \n sign.
        Return a list of trimmed strings.
        '''
        message = [msg]
        while len(message[-1]) > 4096:
            split = message[-1][:4096].rpartition('\n')
            message.append(split[2] + message[-1][4096:])
            message[-2] = split[0]
        return message

    @abstractmethod
    def get_item(self) -> None:
        '''Run Crawler object with specified params, to get and then
        parse item info. Pack them into a temp table.
        '''
        pass

    @abstractmethod
    def compare(self) -> list:
        '''Compare temp table with old record to find new / discount /
        sold / bidding items. Return them in a list by catagory.
        '''
        pass

    @abstractmethod
    def update(self) -> None:
        '''Update old table with new info from temp table. Remember to
        update the "update_time" entry.
        '''
        pass

    @abstractmethod
    def get_message(self) -> list:
        '''Run compare() and generate message for telegram based on
        result.
        '''
        pass


class BlackListManager(ManagerBase):

    def __init__(self, db_path: str=':memory:'):
        super().__init__(db_path)
        self._create_blacklist()

    def _create_blacklist(self) -> None:
        '''Create blacklist table used to filter inproper entries, if
        missing.
        The rule must be stated in a SQL WHERE-like sentence, as a
        string e.g. id <> 1
        '''
        pass

    def _read_rules(self, site: str) -> dict:
        '''Read rules from database into instance.'''
        pass

    def filter(self, site: str, table: str) -> int:
        '''**This method is not safe from injection and should only be
        called when safety is assured!**
        Filter item entries by applying blacklist rules into SQL query
        then deleting hitted entries.
        This should be done before invoking compare() method of any
        subclass.
        '''
        pass

    def add_rule(self, site: str, rule: str) -> bool:
        '''Format: SQL WHERE sentence without WHERE.
        e.g. price >= 20000
        It's possible to launch injection attack via this method. Must
        always be sure input is from reliable source. 
        '''
        pass

    def get_rule(self, rid: int=0, site=None, show_all=False) -> list:
        '''Get and return rules, either by rule id, site name or just
        select all entires.
        Priority: show_all > site > rid.
        '''
        pass

    def remove_rule(self, rid: int) -> bool:
        pass


if __name__ == '__main__':
    logging.basicConfig(format='%(asctime)s - \
    %(name)s - %(levelname)s - %(message)s', level=logging.INFO)